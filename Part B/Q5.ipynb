{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11851119,"sourceType":"datasetVersion","datasetId":7446689},{"sourceId":11899111,"sourceType":"datasetVersion","datasetId":7479889}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key='13b86763ab8ddf529c91c7dce385c6cb04b5253e')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:39:50.689826Z","iopub.execute_input":"2025-05-21T15:39:50.690055Z","iopub.status.idle":"2025-05-21T15:39:59.696197Z","shell.execute_reply.started":"2025-05-21T15:39:50.690022Z","shell.execute_reply":"2025-05-21T15:39:59.695626Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m015\u001b[0m (\u001b[33miitm-ma23m015\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\ntry:\n    import torch, wandb, pandas as pd, numpy as np\nexcept ImportError:\n    !pip install -q torch==2.2.1+cpu torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n    !pip install -q wandb pandas numpy\n    import torch, wandb, pandas as pd, numpy as np\n\nimport random, os, math, json, shutil\nfrom pathlib import Path\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nDATA_DIR = Path(\"/kaggle/input/tamil-translit\")\nTRAIN_F  = DATA_DIR/\"ta.translit.sampled.train.tsv\"\nDEV_F    = DATA_DIR/\"ta.translit.sampled.dev.tsv\"\nTEST_F   = DATA_DIR/\"ta.translit.sampled.test.tsv\"\n\ndef read_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\",\"source\",\"freq\"])\n    df = df.dropna(subset=[\"source\",\"target\"])\n    return [(s, t) for t,s in zip(df.target.astype(str), df.source.astype(str))]\n\ntrain_pairs, dev_pairs, test_pairs = map(read_pairs, [TRAIN_F, DEV_F, TEST_F])\n\n\nclass CharVocab:\n    def __init__(self, seqs):\n        self.char2idx = {'<pad>':0,'<sos>':1,'<eos>':2,'<unk>':3}\n        self.idx2char = ['<pad>','<sos>','<eos>','<unk>']\n        for ch in sorted(set(\"\".join(seqs))):\n            self.char2idx[ch] = len(self.idx2char)\n            self.idx2char.append(ch)\n    def encode(self, txt):   return [self.char2idx.get(c,3) for c in txt]\n    def decode(self, idxs):\n        out=[]; \n        for i in idxs:\n            if i==2: break\n            if i not in (0,1): out.append(self.idx2char[i])\n        return \"\".join(out)\n    def __len__(self): return len(self.idx2char)\n\nsrc_vocab = CharVocab([s for s,_ in train_pairs])\ntgt_vocab = CharVocab([t for _,t in train_pairs])\n\n\nclass TransliterationDS(Dataset):\n    def __init__(self, pairs): self.pairs = pairs\n    def __len__(self): return len(self.pairs)\n    def __getitem__(self, idx):\n        s,t = self.pairs[idx]\n        src = torch.tensor(src_vocab.encode(s), dtype=torch.long)\n        tgt = torch.tensor([1]+tgt_vocab.encode(t)+[2], dtype=torch.long)\n        return src, tgt\ndef collate_fn(batch):\n    src, tgt = zip(*batch)\n    src = pad_sequence(src, batch_first=True, padding_value=0)\n    tgt = pad_sequence(tgt, batch_first=True, padding_value=0)\n    return src, tgt\n\ntrain_ds, dev_ds, test_ds = map(TransliterationDS, [train_pairs, dev_pairs, test_pairs])\n\n\nclass Attention(nn.Module):\n    def __init__(self, hid, attn): super().__init__()\n    def __init__(self, hid_dim, attn_dim):\n        super().__init__()\n        self.W_enc = nn.Linear(hid_dim, attn_dim, bias=False)\n        self.W_dec = nn.Linear(hid_dim, attn_dim, bias=False)\n        self.v     = nn.Linear(attn_dim, 1,      bias=False)\n    def forward(self, enc_out, dec_h):\n        T = enc_out.size(1)\n        dec = self.W_dec(dec_h).unsqueeze(1).expand(-1,T,-1)\n        e   = torch.tanh(self.W_enc(enc_out) + dec)\n        scores = self.v(e).squeeze(-1)             # [B,T]\n        alpha  = torch.softmax(scores, dim=1)\n        ctx    = torch.bmm(alpha.unsqueeze(1), enc_out).squeeze(1)  # [B,H]\n        return ctx, alpha\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb, hid, drop, cell):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n        rnn_cls = {'GRU':nn.GRU,'LSTM':nn.LSTM}[cell]\n        self.rnn = rnn_cls(emb, hid, batch_first=True)\n        self.drop = nn.Dropout(drop)\n        self.cell = cell\n    def forward(self, src):\n        x = self.drop(self.emb(src))\n        out, hid = self.rnn(x)\n        return out, hid\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab, emb, hid, drop, cell, attn_dim):\n        super().__init__()\n        self.emb  = nn.Embedding(vocab, emb, padding_idx=0)\n        self.attn = Attention(hid, attn_dim)\n        rnn_cls   = {'GRU':nn.GRU,'LSTM':nn.LSTM}[cell]\n        self.rnn  = rnn_cls(emb+hid, hid, batch_first=True)\n        self.fc   = nn.Linear(hid*2, vocab)\n        self.drop = nn.Dropout(drop)\n        self.cell = cell\n    def forward(self, tok, hid, enc_out):\n        emb = self.drop(self.emb(tok).unsqueeze(1))  # [B,1,E]\n        dec_h = hid[0] if self.cell=='LSTM' else hid\n        dec_h = dec_h[-1]                            # [B,H]\n        ctx, alpha = self.attn(enc_out, dec_h)       # [B,H]\n        rnn_in = torch.cat([emb, ctx.unsqueeze(1)], dim=2)\n        out, hid = self.rnn(rnn_in, hid)\n        out = out.squeeze(1)\n        pred = self.fc(torch.cat([out, ctx], dim=1))\n        return pred, hid, alpha\n\nclass Seq2SeqAttn(nn.Module):\n    def __init__(self, cfg, device):\n        super().__init__()\n        self.device = device\n        self.encoder = Encoder(len(src_vocab), cfg.emb_dim, cfg.hid_dim,\n                               cfg.dropout, cfg.cell)\n        self.decoder = Decoder(len(tgt_vocab), cfg.emb_dim, cfg.hid_dim,\n                               cfg.dropout, cfg.cell, cfg.attn_dim)\n        self.tgt_size = len(tgt_vocab)\n        self.cell = cfg.cell\n    def forward(self, src, tgt, teacher=0.5):\n        B,T = tgt.shape\n        outs = torch.zeros(B,T,self.tgt_size, device=self.device)\n        enc_out, hid = self.encoder(src)\n        tok = tgt[:,0]\n        for t in range(1,T):\n            pred, hid, _ = self.decoder(tok, hid, enc_out)\n            outs[:,t] = pred\n            tok = tgt[:,t] if random.random() < teacher else pred.argmax(1)\n        return outs\n    @torch.no_grad()\n    def greedy(self, src, max_len=50):\n        self.eval()\n        enc_out, hid = self.encoder(src)\n        tok = torch.full((src.size(0),), tgt_vocab.char2idx['<sos>'],\n                         dtype=torch.long, device=src.device)\n        preds=[]\n        for _ in range(max_len):\n            pred, hid, _ = self.decoder(tok, hid, enc_out)\n            tok = pred.argmax(1)\n            preds.append(tok)\n        preds = torch.stack(preds,1)\n        texts=[tgt_vocab.decode(row.cpu().numpy()) for row in preds]\n        return texts\n\n\ndef train_epoch(model, loader, opt, crit, device):\n    model.train(); tot=0\n    for src,tgt in loader:\n        src,tgt = src.to(device), tgt.to(device)\n        opt.zero_grad()\n        out = model(src,tgt,teacher=0.5)\n        loss = crit(out[:,1:].reshape(-1, model.tgt_size),\n                    tgt[:,1:].reshape(-1))\n        loss.backward(); opt.step(); tot+=loss.item()\n    return tot/len(loader)\n\n@torch.no_grad()\ndef word_acc(model, loader, device):\n    model.eval(); ok=tot=0\n    for src,tgt in loader:\n        src,tgt = src.to(device), tgt.to(device)\n        out = model(src,tgt,teacher=0)\n        pred = out.argmax(2)\n        for p,t in zip(pred.cpu(), tgt.cpu()):\n            if tgt_vocab.decode(p.numpy()) == tgt_vocab.decode(t.numpy()):\n                ok+=1\n            tot+=1\n    return ok/tot\n\ndef eval_loss(model, loader, crit, device):\n    model.eval(); tot=0\n    for src,tgt in loader:\n        src,tgt = src.to(device), tgt.to(device)\n        out = model(src,tgt,teacher=0)\n        loss = crit(out[:,1:].reshape(-1, model.tgt_size),\n                    tgt[:,1:].reshape(-1))\n        tot+=loss.item()\n    return tot/len(loader)\n\n\nsweep_cfg = {\n    \"method\":\"bayes\",\n    \"name\":\"attn-sweep\",\n    \"metric\":{\"name\":\"validation_accuracy\",\"goal\":\"maximize\"},\n    \"parameters\":{\n        \"emb_dim\":{\"values\":[64,128,256]},\n        \"hid_dim\":{\"values\":[64,128,256]},\n        \"attn_dim\":{\"values\":[128,256]},\n        \"dropout\":{\"values\":[0.1,0.3]},\n        \"cell\":{\"values\":[\"GRU\",\"LSTM\"]}\n    }\n}\nsweep_id = wandb.sweep(sweep_cfg, project=\"MA23M015_DL_Assignment3\")\n\ndef sweep_run():\n    with wandb.init() as run:\n        cfg = wandb.config\n        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        dl_train=DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n        dl_dev  =DataLoader(dev_ds,   batch_size=64, shuffle=False,collate_fn=collate_fn)\n        dl_test =DataLoader(test_ds,  batch_size=64, shuffle=False,collate_fn=collate_fn)\n\n        class C: pass\n        C.emb_dim=cfg.emb_dim; C.hid_dim=cfg.hid_dim; C.dropout=cfg.dropout\n        C.attn_dim=cfg.attn_dim; C.cell=cfg.cell\n        model=Seq2SeqAttn(C, device).to(device)\n        opt=torch.optim.Adam(model.parameters())\n        crit=nn.CrossEntropyLoss(ignore_index=0)\n\n        best=0\n        for ep in range(1,11):\n            tr_loss=train_epoch(model, dl_train, opt, crit, device)\n            tr_acc =word_acc(model, dl_train, device)\n            val_loss=eval_loss(model, dl_dev, crit, device)\n            val_acc =word_acc(model, dl_dev, device)\n            wandb.log({\"epoch\": ep, \"train_loss\": tr_loss, \"train_acc\": tr_acc,\n                       \"validation_loss\": val_loss, \"validation_accuracy\": val_acc})\n            print(f\"epoch:{ep:02d} train_loss:{tr_loss:.3f} train_acc:{tr_acc:.3f} \"\n                  f\"val_loss:{val_loss:.3f} val_acc{val_acc:.3f}\")\n            if val_acc > best:\n                best = val_acc\n                torch.save(model.state_dict(), \"best_model.pt\")\n\n        # Final test evaluation using the best model\n        model.load_state_dict(torch.load(\"best_model.pt\"))\n        test_acc = word_acc(model, dl_test, device)\n        wandb.log({\"test_accuracy\": test_acc})\n\n        print(f\"BEST val_acc {best*100:.2f}%  →  test_acc {test_acc*100:.2f}%\")\n\n        # save predictions\n        preds=[]\n        for src,_ in dl_test:\n            src=src.to(device)\n            txt=model.greedy(src)\n            preds.extend(txt)\n        out_dir=Path(\"predictions_attention\"); out_dir.mkdir(exist_ok=True)\n        pd.DataFrame({\"source\":[s for s,_ in test_pairs],\n                      \"prediction\":preds}\n                    ).to_csv(out_dir/\"test_predictions.tsv\",sep=\"\\t\",index=False)\n        wandb.save(str(out_dir/\"test_predictions.tsv\"))\n\n\nwandb.agent(sweep_id, function=sweep_run, count=2)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:40:34.389494Z","iopub.execute_input":"2025-05-21T15:40:34.390046Z","iopub.status.idle":"2025-05-21T16:03:21.936802Z","shell.execute_reply.started":"2025-05-21T15:40:34.390024Z","shell.execute_reply":"2025-05-21T16:03:21.936102Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: bhtzulgd\nSweep URL: https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i127tr8q with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thid_dim: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_154046-i127tr8q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/i127tr8q' target=\"_blank\">different-sweep-1</a></strong> to <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/i127tr8q' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/i127tr8q</a>"},"metadata":{}},{"name":"stdout","text":"epoch:01 train_loss:0.948 train_acc:0.427 val_loss:0.876 val_acc0.411\nepoch:02 train_loss:0.467 train_acc:0.545 val_loss:0.738 val_acc0.511\nepoch:03 train_loss:0.399 train_acc:0.568 val_loss:0.730 val_acc0.506\nepoch:04 train_loss:0.360 train_acc:0.616 val_loss:0.716 val_acc0.540\nepoch:05 train_loss:0.328 train_acc:0.639 val_loss:0.678 val_acc0.560\nepoch:06 train_loss:0.304 train_acc:0.670 val_loss:0.690 val_acc0.564\nepoch:07 train_loss:0.289 train_acc:0.684 val_loss:0.694 val_acc0.566\nepoch:08 train_loss:0.274 train_acc:0.683 val_loss:0.730 val_acc0.559\nepoch:09 train_loss:0.262 train_acc:0.713 val_loss:0.718 val_acc0.572\nepoch:10 train_loss:0.251 train_acc:0.726 val_loss:0.699 val_acc0.576\nBEST val_acc 57.61%  →  test_acc 55.32%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▅▆▇██▇██</td></tr><tr><td>validation_loss</td><td>█▃▃▂▁▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.55318</td></tr><tr><td>train_acc</td><td>0.72579</td></tr><tr><td>train_loss</td><td>0.2514</td></tr><tr><td>validation_accuracy</td><td>0.57609</td></tr><tr><td>validation_loss</td><td>0.69933</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">different-sweep-1</strong> at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/i127tr8q' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/i127tr8q</a><br> View project at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_154046-i127tr8q/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7re6hdui with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thid_dim: 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_155200-7re6hdui</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">atomic-sweep-2</a></strong> to <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui</a>"},"metadata":{}},{"name":"stdout","text":"epoch:01 train_loss:1.297 train_acc:0.296 val_loss:1.110 val_acc0.282\nepoch:02 train_loss:0.652 train_acc:0.400 val_loss:0.941 val_acc0.366\nepoch:03 train_loss:0.542 train_acc:0.468 val_loss:0.823 val_acc0.449\nepoch:04 train_loss:0.488 train_acc:0.506 val_loss:0.801 val_acc0.475\nepoch:05 train_loss:0.454 train_acc:0.524 val_loss:0.760 val_acc0.491\nepoch:06 train_loss:0.426 train_acc:0.549 val_loss:0.731 val_acc0.505\nepoch:07 train_loss:0.408 train_acc:0.570 val_loss:0.732 val_acc0.516\nepoch:08 train_loss:0.394 train_acc:0.576 val_loss:0.737 val_acc0.516\nepoch:09 train_loss:0.381 train_acc:0.592 val_loss:0.738 val_acc0.519\nepoch:10 train_loss:0.368 train_acc:0.601 val_loss:0.709 val_acc0.533\nBEST val_acc 53.27%  →  test_acc 51.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▆▆▇▇████</td></tr><tr><td>validation_loss</td><td>█▅▃▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.51501</td></tr><tr><td>train_acc</td><td>0.60086</td></tr><tr><td>train_loss</td><td>0.36826</td></tr><tr><td>validation_accuracy</td><td>0.53274</td></tr><tr><td>validation_loss</td><td>0.70939</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">atomic-sweep-2</strong> at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui</a><br> View project at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_155200-7re6hdui/logs</code>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\nimport random, torch, numpy as np, pandas as pd, seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom pathlib import Path\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom io import BytesIO\nfrom PIL import Image\nimport wandb\n\n\nSEED = 42\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n\nfont_path = Path(\"/kaggle/input/notosans-tamil/NotoSansTamil-VariableFont_wdth,wght.ttf\")\nif font_path.exists():\n    fm.fontManager.addfont(str(font_path))\n    plt.rcParams[\"font.family\"] = \"Noto Sans Tamil\"\n\n\n\nDATA_DIR = Path(\"/kaggle/input/tamil-translit\")\nTRAIN_F  = DATA_DIR / \"ta.translit.sampled.train.tsv\"\nDEV_F    = DATA_DIR / \"ta.translit.sampled.dev.tsv\"\nTEST_F   = DATA_DIR / \"ta.translit.sampled.test.tsv\"\n\n\ndef read_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None,\n                     names=[\"target\", \"source\", \"freq\"])\n    df = df.dropna(subset=[\"source\", \"target\"])\n    return [(s, t) for t, s in zip(df.target.astype(str),\n                                   df.source.astype(str))]\n\ntrain_pairs, dev_pairs, test_pairs = map(read_pairs,\n                                         [TRAIN_F, DEV_F, TEST_F])\n\nclass CharVocab:\n    def __init__(self, seqs):\n        self.char2idx = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n        self.idx2char = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n        for ch in sorted(set(\"\".join(seqs))):\n            self.char2idx[ch] = len(self.idx2char)\n            self.idx2char.append(ch)\n    def encode(self, txt):   return [self.char2idx.get(c, 3) for c in txt]\n    def decode(self, idxs):\n        out = []\n        for i in idxs:\n            if i == 2: break\n            if i not in (0, 1): out.append(self.idx2char[i])\n        return \"\".join(out)\n    def __len__(self): return len(self.idx2char)\n\nsrc_vocab = CharVocab([s for s, _ in train_pairs])\ntgt_vocab = CharVocab([t for _, t in train_pairs])\n\nclass TransliterationDS(Dataset):\n    def __init__(self, pairs): self.pairs = pairs\n    def __len__(self): return len(self.pairs)\n    def __getitem__(self, idx):\n        s, t = self.pairs[idx]\n        src = torch.tensor(src_vocab.encode(s), dtype=torch.long)\n        tgt = torch.tensor([1] + tgt_vocab.encode(t) + [2], dtype=torch.long)\n        return src, tgt, s, t   # keep the raw strings for later\n\ndef collate_fn(batch):\n    src, tgt, s_str, t_str = zip(*batch)\n    src = pad_sequence(src, batch_first=True, padding_value=0)\n    tgt = pad_sequence(tgt, batch_first=True, padding_value=0)\n    return src, tgt, s_str, t_str\n\n\nclass Attention(nn.Module):\n    def __init__(self, hid_dim, attn_dim):\n        super().__init__()\n        self.W_enc = nn.Linear(hid_dim, attn_dim, bias=False)\n        self.W_dec = nn.Linear(hid_dim, attn_dim, bias=False)\n        self.v     = nn.Linear(attn_dim, 1,      bias=False)\n    def forward(self, enc_out, dec_h):\n        # enc_out : [B,T,H]   dec_h : [B,H]\n        T     = enc_out.size(1)\n        dec   = self.W_dec(dec_h).unsqueeze(1).expand(-1, T, -1)\n        e     = torch.tanh(self.W_enc(enc_out) + dec)   # [B,T,A]\n        score = self.v(e).squeeze(-1)                   # [B,T]\n        alpha = torch.softmax(score, dim=1)             # attention weights\n        ctx   = torch.bmm(alpha.unsqueeze(1), enc_out).squeeze(1)  # [B,H]\n        return ctx, alpha\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb, hid, drop, cell):\n        super().__init__()\n        self.emb  = nn.Embedding(vocab, emb, padding_idx=0)\n        rnn_cls   = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell]\n        self.rnn  = rnn_cls(emb, hid, batch_first=True)\n        self.drop = nn.Dropout(drop)\n        self.cell = cell\n    def forward(self, src):\n        x = self.drop(self.emb(src))\n        out, hid = self.rnn(x)\n        return out, hid\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab, emb, hid, drop, cell, attn_dim):\n        super().__init__()\n        self.emb  = nn.Embedding(vocab, emb, padding_idx=0)\n        self.attn = Attention(hid, attn_dim)\n        rnn_cls   = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell]\n        self.rnn  = rnn_cls(emb + hid, hid, batch_first=True)\n        self.fc   = nn.Linear(hid * 2, vocab)\n        self.drop = nn.Dropout(drop)\n        self.cell = cell\n    def forward(self, tok, hid, enc_out):\n        emb  = self.drop(self.emb(tok).unsqueeze(1))        # [B,1,E]\n        dec_h = hid[0] if self.cell == \"LSTM\" else hid\n        dec_h = dec_h[-1]                                   # [B,H]\n        ctx, alpha = self.attn(enc_out, dec_h)              # [B,H]\n        rnn_in = torch.cat([emb, ctx.unsqueeze(1)], dim=2)  # [B,1,E+H]\n        out, hid = self.rnn(rnn_in, hid)\n        out = out.squeeze(1)                                # [B,H]\n        pred = self.fc(torch.cat([out, ctx], dim=1))        # [B,V]\n        return pred, hid, alpha\n\nclass Seq2SeqAttn(nn.Module):\n    def __init__(self, cfg, device):\n        super().__init__()\n        self.device  = device\n        self.encoder = Encoder(len(src_vocab), cfg.emb_dim,\n                               cfg.hid_dim, cfg.dropout, cfg.cell)\n        self.decoder = Decoder(len(tgt_vocab), cfg.emb_dim,\n                               cfg.hid_dim, cfg.dropout, cfg.cell,\n                               cfg.attn_dim)\n        self.tgt_size = len(tgt_vocab)\n        self.cell     = cfg.cell\n\n    def forward(self, src, tgt, teacher=0.5):\n        B, T = tgt.shape\n        outs = torch.zeros(B, T, self.tgt_size, device=self.device)\n        enc_out, hid = self.encoder(src)\n        tok = tgt[:, 0]      # <sos>\n        for t in range(1, T):\n            pred, hid, _ = self.decoder(tok, hid, enc_out)\n            outs[:, t] = pred\n            use_gt = random.random() < teacher\n            tok = tgt[:, t] if use_gt else pred.argmax(1)\n        return outs\n\n    @torch.no_grad()\n    def greedy_with_attention(self, src, max_len=50):\n        self.eval()\n        enc_out, hid = self.encoder(src)\n        tok = torch.full((src.size(0),),\n                         tgt_vocab.char2idx[\"<sos>\"],\n                         dtype=torch.long, device=src.device)\n        preds, attentions = [], []\n        for _ in range(max_len):\n            pred, hid, alpha = self.decoder(tok, hid, enc_out)\n            tok = pred.argmax(1)\n            preds.append(tok)\n            attentions.append(alpha)\n            if (tok == tgt_vocab.char2idx[\"<eos>\"]).all(): break\n        preds      = torch.stack(preds, 1)          # [B,T_tgt]\n        attentions = torch.stack(attentions, 1)     # [B,T_tgt,T_src]\n        texts = [tgt_vocab.decode(row.cpu().numpy()) for row in preds]\n        return texts, attentions\n\ndef train_epoch(model, loader, opt, crit, device):\n    model.train()\n    tot = 0\n    for src, tgt, _, _ in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        opt.zero_grad()\n        out = model(src, tgt, teacher=0.5)\n        loss = crit(out[:, 1:].reshape(-1, model.tgt_size),\n                    tgt[:, 1:].reshape(-1))\n        loss.backward()\n        opt.step()\n        tot += loss.item()\n    return tot / len(loader)\n\n@torch.no_grad()\ndef word_acc(model, loader, device):\n    model.eval()\n    ok = tot = 0\n    for src, tgt, _, _ in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        out = model(src, tgt, teacher=0)\n        pred = out.argmax(2)\n        for p, t in zip(pred.cpu(), tgt.cpu()):\n            if tgt_vocab.decode(p.numpy()) == tgt_vocab.decode(t.numpy()):\n                ok += 1\n            tot += 1\n    return ok / tot\n\n@torch.no_grad()\ndef eval_loss(model, loader, crit, device):\n    model.eval()\n    tot = 0\n    for src, tgt, _, _ in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        out = model(src, tgt, teacher=0)\n        loss = crit(out[:, 1:].reshape(-1, model.tgt_size),\n                    tgt[:, 1:].reshape(-1))\n        tot += loss.item()\n    return tot / len(loader)\n\n\nclass Config:\n    def __init__(self):\n        self.emb_dim  = 128\n        self.hid_dim  = 256\n        self.attn_dim = 256\n        self.dropout  = 0.3\n        self.cell     = \"LSTM\"\n\n\nwandb.init(project=\"MA23M015_DL_Assignment3\",\n           name=\"attention-visualization-lstm\",\n           config={\n               \"emb_dim\": 128,\n               \"hid_dim\": 256,\n               \"attn_dim\": 256,\n               \"dropout\": 0.3,\n               \"cell\": \"LSTM\",\n               \"seed\": SEED\n           })\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[INFO] ‑ Using device: {device}\")\n\ntrain_ds = TransliterationDS(train_pairs)\ndev_ds   = TransliterationDS(dev_pairs)\ntest_ds  = TransliterationDS(test_pairs)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n                          collate_fn=collate_fn)\ndev_loader   = DataLoader(dev_ds,   batch_size=64, shuffle=False,\n                          collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=1,  shuffle=False,\n                          collate_fn=collate_fn)\n\n\ncfg   = Config()\nmodel = Seq2SeqAttn(cfg, device).to(device)\nopt   = torch.optim.Adam(model.parameters())\ncrit  = nn.CrossEntropyLoss(ignore_index=0)\n\n\nEPOCHS        = 10\nbest_val_acc  = 0.0\nMODEL_PATH    = \"transliteration_model.pt\"\n\n\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss = train_epoch(model, train_loader, opt, crit, device)\n    tr_acc  = word_acc(model, train_loader, device)\n    val_loss = eval_loss(model, dev_loader, crit, device)\n    val_acc  = word_acc(model, dev_loader, device)\n\n    wandb.log({\n        \"epoch\": epoch,\n        \"train_loss\": tr_loss,\n        \"train_acc\":  tr_acc,\n        \"val_loss\": val_loss,\n        \"val_acc\":  val_acc\n    })\n\n    print(f\"Epoch {epoch:02d}: \"\n          f\"Train‑Loss {tr_loss:.4f}  Train‑Acc {tr_acc:.4f} | \"\n          f\"Val‑Loss {val_loss:.4f}  Val‑Acc {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"[INFO] ‑ New best model saved (Val‑Acc = {val_acc:.4f})\")\n\n\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.eval()\n\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nimport wandb\n\nfigs  = []\ncount = 0\n\nsample_ids = random.sample(range(len(test_ds)), 9)\n\nfor idx in sample_ids:\n    src_tok, _, src_str, _ = test_ds[idx]\n    src_tensor = src_tok.unsqueeze(0).to(device)\n    pred_texts, attn = model.greedy_with_attention(src_tensor)\n    pred_str    = pred_texts[0]\n\n    attn_mat = attn.squeeze(0).cpu().numpy()[:len(pred_str), :len(src_str)]\n\n    # Plot single heatmap \n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(attn_mat,\n                ax=ax, cmap=\"viridis\",\n                cbar=False,\n                xticklabels=list(src_str),\n                yticklabels=list(pred_str))\n    ax.set_xlabel(\"Source\")\n    ax.set_ylabel(\"Predicted\")\n    ax.set_title(f\"{src_str} -> {pred_str}\")\n    plt.tight_layout()\n\n  \n    figs.append(fig)\n    count += 1\n\ngrid_fig, grid_axes = plt.subplots(3, 3, figsize=(18, 18))\n\nfor i, (ax, single_fig) in enumerate(zip(grid_axes.ravel(), figs)):\n    single_fig.canvas.draw()\n    img = np.asarray(single_fig.canvas.buffer_rgba())\n    ax.imshow(img)\n    ax.axis(\"off\")\n    ax.set_title(f\"Sample {i+1}\", fontsize=14)\n\nplt.tight_layout()\n\nbuf = BytesIO()\ngrid_fig.savefig(buf, format=\"png\", dpi=150)\nbuf.seek(0)\nwandb.log({\"attention_grid\": wandb.Image(Image.open(buf))})\n\n\nplt.close(grid_fig)\nfor f in figs:\n    plt.close(f)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:47:27.485090Z","iopub.execute_input":"2025-05-21T16:47:27.485821Z","iopub.status.idle":"2025-05-21T17:00:05.903323Z","shell.execute_reply.started":"2025-05-21T16:47:27.485795Z","shell.execute_reply":"2025-05-21T17:00:05.902789Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'MA23M015_DL_Assignment3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attention-visualization-lstm</strong> at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui</a><br> View project at: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_164618-7re6hdui/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_164727-7re6hdui</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">attention-visualization-lstm</a></strong> to <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/sweeps/bhtzulgd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui' target=\"_blank\">https://wandb.ai/iitm-ma23m015/MA23M015_DL_Assignment3/runs/7re6hdui</a>"},"metadata":{}},{"name":"stdout","text":"[INFO] ‑ Using device: cuda\nEpoch 01: Train‑Loss 0.8536  Train‑Acc 0.5113 | Val‑Loss 0.7628  Val‑Acc 0.4831\n[INFO] ‑ New best model saved (Val‑Acc = 0.4831)\nEpoch 02: Train‑Loss 0.4225  Train‑Acc 0.5843 | Val‑Loss 0.7359  Val‑Acc 0.5260\n[INFO] ‑ New best model saved (Val‑Acc = 0.5260)\nEpoch 03: Train‑Loss 0.3544  Train‑Acc 0.6310 | Val‑Loss 0.6799  Val‑Acc 0.5660\n[INFO] ‑ New best model saved (Val‑Acc = 0.5660)\nEpoch 04: Train‑Loss 0.3139  Train‑Acc 0.6763 | Val‑Loss 0.6959  Val‑Acc 0.5827\n[INFO] ‑ New best model saved (Val‑Acc = 0.5827)\nEpoch 05: Train‑Loss 0.2846  Train‑Acc 0.6957 | Val‑Loss 0.6944  Val‑Acc 0.5726\nEpoch 06: Train‑Loss 0.2568  Train‑Acc 0.7282 | Val‑Loss 0.7299  Val‑Acc 0.5784\nEpoch 07: Train‑Loss 0.2382  Train‑Acc 0.7507 | Val‑Loss 0.7368  Val‑Acc 0.5869\n[INFO] ‑ New best model saved (Val‑Acc = 0.5869)\nEpoch 08: Train‑Loss 0.2171  Train‑Acc 0.7739 | Val‑Loss 0.7101  Val‑Acc 0.5871\n[INFO] ‑ New best model saved (Val‑Acc = 0.5871)\nEpoch 09: Train‑Loss 0.2021  Train‑Acc 0.7912 | Val‑Loss 0.7447  Val‑Acc 0.5935\n[INFO] ‑ New best model saved (Val‑Acc = 0.5935)\nEpoch 10: Train‑Loss 0.1888  Train‑Acc 0.8087 | Val‑Loss 0.7568  Val‑Acc 0.5834\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}