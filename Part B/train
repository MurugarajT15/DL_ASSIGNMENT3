# train.py

import argparse
import torch
from torch.utils.data import DataLoader
import wandb
from data import load_datasets
from model.seq2seq_attn import Seq2SeqAttn
from heatmap import HeatmapCell

# Placeholder vocabulary setup
# In practice, use torchtext or build your own Vocab class
from torch.nn.utils.rnn import pad_sequence
from collections import defaultdict
from itertools import chain

class Vocab:
    def __init__(self, tokens):
        self.itos = ["<pad>", "<sos>", "<eos>", "<unk>"] + sorted(set(tokens))
        self.stoi = {s: i for i, s in enumerate(self.itos)}

    def numericalize(self, text):
        return [self.stoi.get("<sos>")] + [self.stoi.get(ch, self.stoi["<unk>"]) for ch in text] + [self.stoi.get("<eos>")]

    def __len__(self):
        return len(self.itos)

def collate_fn(batch, input_vocab, output_vocab):
    src_batch, tgt_batch = [], []
    for src, tgt in batch:
        src_batch.append(torch.tensor(input_vocab.numericalize(src), dtype=torch.long))
        tgt_batch.append(torch.tensor(output_vocab.numericalize(tgt), dtype=torch.long))
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    src_lengths = torch.tensor([len(x) for x in src_batch])
    return src_padded, src_lengths, tgt_padded

def train_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for src, src_lengths, tgt in dataloader:
        src, src_lengths, tgt = src.to(device), src_lengths.to(device), tgt.to(device)
        optimizer.zero_grad()
        output = model(src, src_lengths, tgt)
        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    wandb.init(project="tamil-transliteration", config=vars(args))

    train_pairs, dev_pairs, _ = load_datasets(args.data_path)
    input_vocab = Vocab("".join(src for src, _ in train_pairs))
    output_vocab = Vocab("".join(tgt for _, tgt in train_pairs))

    train_loader = DataLoader(train_pairs, batch_size=args.batch_size, shuffle=True,
                              collate_fn=lambda b: collate_fn(b, input_vocab, output_vocab))

    model = Seq2SeqAttn(len(input_vocab), len(output_vocab), args.embed_size,
                        args.hidden_size, args.num_layers, args.dropout, args.cell_type).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)

    for epoch in range(args.epochs):
        loss = train_epoch(model, train_loader, optimizer, criterion, device)
        wandb.log({"train_loss": loss, "epoch": epoch + 1})
        print(f"Epoch {epoch + 1} - Loss: {loss:.4f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, default=".", help="Path to dataset folder")
    parser.add_argument("--embed_size", type=int, default=256)
    parser.add_argument("--hidden_size", type=int, default=256)
    parser.add_argument("--num_layers", type=int, default=3)
    parser.add_argument("--dropout", type=float, default=0.3)
    parser.add_argument("--cell_type", type=str, choices=["RNN", "LSTM", "GRU"], default="LSTM")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--epochs", type=int, default=10)
    args = parser.parse_args()
    main(args)
