# seq2seq_attn.py

import torch
import torch.nn as nn
from model.attention import EncoderRNN, AttnDecoderRNN

class Seq2SeqAttn(nn.Module):
    def __init__(self, input_vocab_size, output_vocab_size, embed_size, hidden_size,
                 num_layers=1, dropout=0.0, cell_type="LSTM"):
        super(Seq2SeqAttn, self).__init__()
        self.encoder = EncoderRNN(input_vocab_size, embed_size, hidden_size,
                                  n_layers=num_layers, cell_type=cell_type, dropout=dropout)
        self.decoder = AttnDecoderRNN(output_vocab_size, embed_size, hidden_size,
                                      n_layers=num_layers, cell_type=cell_type, dropout=dropout)

    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len = trg.size()
        outputs = []
        encoder_outputs, hidden = self.encoder(src, src_lengths)
        input = trg[:, 0]  # <sos>

        for t in range(1, trg_len):
            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, src != 0)
            outputs.append(output.unsqueeze(1))
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1

        return torch.cat(outputs, dim=1)
