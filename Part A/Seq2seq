import torch
import torch.nn as nn
import torch.nn.functional as F

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, sos_idx, eos_idx, pad_idx, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.sos_idx = sos_idx
        self.eos_idx = eos_idx
        self.pad_idx = pad_idx
        self.device = device

    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):
        batch_size, max_len = tgt.size()
        vocab_size = self.decoder.output_size

        outputs = torch.zeros(batch_size, max_len, vocab_size).to(self.device)

        encoder_outputs, hidden = self.encoder(src, src_lengths)

        input = tgt[:, 0]

        for t in range(1, max_len):
            output, hidden = self.decoder(input, hidden, encoder_outputs)
            outputs[:, t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = tgt[:, t] if teacher_force else top1

        return outputs, hidden

    def greedy_decode(self, src, src_lengths, max_len=30):
        self.eval()
        batch_size = src.size(0)
        outputs = torch.zeros(batch_size, max_len).long().to(self.device)

        with torch.no_grad():
            encoder_outputs, hidden = self.encoder(src, src_lengths)
            input = torch.tensor([self.sos_idx] * batch_size).to(self.device)

            for t in range(max_len):
                output, hidden = self.decoder(input, hidden, encoder_outputs)
                top1 = output.argmax(1)
                outputs[:, t] = top1
                input = top1

        return outputs
