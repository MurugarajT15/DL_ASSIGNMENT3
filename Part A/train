import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from data import CharVocab, TranslitDataset, pad_collate
from model import Encoder, Decoder, BahdanauAttention
from seq2seq import Seq2Seq
import wandb


def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0

    for src, src_lens, tgt, tgt_lens in iterator:
        src, tgt = src.to(device), tgt.to(device)
        optimizer.zero_grad()
        output, _ = model(src, src_lens, tgt, teacher_forcing_ratio=args.teacher_forcing)
        output_dim = output.shape[-1]

        output = output[:, 1:].reshape(-1, output_dim)
        tgt = tgt[:, 1:].reshape(-1)
        loss = criterion(output, tgt)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()

    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src, src_lens, tgt, tgt_lens in iterator:
            src, tgt = src.to(device), tgt.to(device)
            output, _ = model(src, src_lens, tgt, teacher_forcing_ratio=0)
            output_dim = output.shape[-1]
            output = output[:, 1:].reshape(-1, output_dim)
            tgt = tgt[:, 1:].reshape(-1)
            loss = criterion(output, tgt)
            epoch_loss += loss.item()

    return epoch_loss / len(iterator)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_path', type=str, required=True)
    parser.add_argument('--dev_path', type=str, required=True)
    parser.add_argument('--test_path', type=str, required=True)
    parser.add_argument('--embedding_size', type=int, default=64)
    parser.add_argument('--hidden_size', type=int, default=128)
    parser.add_argument('--enc_layers', type=int, default=1)
    parser.add_argument('--dec_layers', type=int, default=1)
    parser.add_argument('--cell_type', type=str, choices=['RNN', 'GRU', 'LSTM'], default='GRU')
    parser.add_argument('--dropout', type=float, default=0.3)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--teacher_forcing', type=float, default=0.5)
    parser.add_argument('--attention', action='store_true')
    parser.add_argument('--wandb', action='store_true')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    vocab = CharVocab([args.train_path, args.dev_path, args.test_path])
    train_data = TranslitDataset(args.train_path, vocab)
    dev_data = TranslitDataset(args.dev_path, vocab)

    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, collate_fn=pad_collate)
    dev_loader = DataLoader(dev_data, batch_size=args.batch_size, collate_fn=pad_collate)

    attention = BahdanauAttention(args.hidden_size) if args.attention else None
    encoder = Encoder(len(vocab), args.embedding_size, args.hidden_size, args.enc_layers, args.cell_type, args.dropout)
    decoder = Decoder(len(vocab), args.embedding_size, args.hidden_size, args.dec_layers, args.cell_type, args.dropout, attention)
    model = Seq2Seq(encoder, decoder, vocab.sos_idx, vocab.eos_idx, vocab.pad_idx, device).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)

    if args.wandb:
        wandb.init(project="tamil-translit", config=vars(args))
        wandb.watch(model)

    best_loss = float('inf')
    for epoch in range(args.epochs):
        train_loss = train(model, train_loader, optimizer, criterion, 1.0)
        dev_loss = evaluate(model, dev_loader, criterion)

        if args.wandb:
            wandb.log({"Train Loss": train_loss, "Val Loss": dev_loss})

        print(f"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Val Loss: {dev_loss:.3f}")

        if dev_loss < best_loss:
            best_loss = dev_loss
            torch.save(model.state_dict(), "best_model.pt")
