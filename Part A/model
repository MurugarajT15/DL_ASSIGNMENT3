class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, cell_type, pad_idx, bidirectional=False):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)
        self.dropout = nn.Dropout(dropout)
        self.cell_type = cell_type.upper()
        self.n_layers = n_layers
        self.hid_dim = hid_dim
        self.bidirectional = bidirectional
        self.n_directions = 2 if bidirectional else 1
        
        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]
        self.rnn = rnn_cls(
            emb_dim, 
            hid_dim, 
            n_layers, 
            batch_first=True, 
            dropout=dropout if n_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
    def forward(self, src):
        embedded = self.dropout(self.embedding(src))
        outputs, hidden = self.rnn(embedded)
        
        # Process hidden state based on RNN type
        if self.cell_type == 'LSTM':
            
            h_n, c_n = hidden          
            if self.bidirectional:
 
                h_n = h_n.view(self.n_layers, self.n_directions, -1, self.hid_dim)
                h_n = h_n.sum(dim=1)  # Sum the bidirectional outputs
                
                c_n = c_n.view(self.n_layers, self.n_directions, -1, self.hid_dim)
                c_n = c_n.sum(dim=1)  # Sum the bidirectional outputs
                
            return (h_n, c_n)
        else:
            if self.bidirectional:
                hidden = hidden.view(self.n_layers, self.n_directions, -1, self.hid_dim)
                hidden = hidden.sum(dim=1)  
                
            return hidden

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, cell_type, pad_idx):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)
        self.dropout = nn.Dropout(dropout)
        self.cell_type = cell_type.upper()
        self.n_layers = n_layers
        self.hid_dim = hid_dim
        
        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]
        self.rnn = rnn_cls(
            emb_dim, 
            hid_dim, 
            n_layers, 
            batch_first=True, 
            dropout=dropout if n_layers > 1 else 0
        )
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
    def forward(self, input, hidden):
        input = input.unsqueeze(1)
        embedded = self.dropout(self.embedding(input))
        output, hidden = self.rnn(embedded, hidden)
        prediction = self.fc_out(output.squeeze(1))
        return prediction, hidden
